from json import dumps
from os import getenv, system
from threading import Thread
from time import time
from typing import Dict, List, Union
from typing import Tuple
from uuid import uuid4

import gradio as gr
import torch
from fastapi import FastAPI
from flasgger import Schema, fields
from flask import Flask, Blueprint, Response, current_app, request, stream_with_context
from flask_cors import CORS
from marshmallow import validate
from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedModel, PreTrainedTokenizer

from utils import *

blueprint = Blueprint(name="Chat", import_name=__name__, url_prefix="/v1/chat")


# ä½¿ç”¨marshmallowåšåºåˆ—åŒ–å’Œå‚æ•°æ ¡éªŒ

class ChatMessageSchema(Schema):
    """Chatæ¶ˆæ¯ç»“æ„æ˜ å°„"""
    role = fields.Str(required=True)
    content = fields.Str(required=True)


class ChatDeltaSchema(Schema):
    """Chatæµå¼ç»“æ„æ˜ å°„"""
    role = fields.Str()
    content = fields.Str()


class ChatRequestSchema(Schema):
    """Chatæ¥å£è¯·æ±‚æ•°æ®ç»“æ„è§£æ"""
    model = fields.Str(required=True)  # noqa
    messages = fields.List(fields.Nested(nested=ChatMessageSchema), required=True)  # noqa
    stream = fields.Bool(load_default=True)
    max_tokens = fields.Int(load_default=None)
    n = fields.Int(load_default=1)
    seed = fields.Int(load_default=1)
    top_p = fields.Float(load_default=1.0)
    temperature = fields.Float(load_default=1.0)
    presence_penalty = fields.Float(load_default=0.0)
    frequency_penalty = fields.Float(load_default=0.0)


class ChatChoiceSchema(Schema):
    """Chatæµå¼æ¶ˆæ¯é€‰æ‹©å™¨"""
    index = fields.Int(load_default=0)
    delta = fields.Nested(nested=ChatDeltaSchema)  # noqa
    finish_reason = fields.Str(
        validate=validate.OneOf(["stop", "length", "content_filter", "function_call"]),  # noqa
        metadata={"example": "stop"})


class ChatResponseSchema(Schema):
    """Chatæ¥å£å“åº”æ•°æ®ç»“æ„æ˜ å°„"""
    id = fields.Str(dump_default=lambda: uuid4().hex)
    created = fields.Int(dump_default=lambda: int(time()))
    model = fields.Str(required=True)  # noqa
    choices = fields.List(fields.Nested(nested=ChatChoiceSchema))  # noqa
    object = fields.Constant(constant="chat.completion.chunk")


def init_model_and_tokenizer() -> Tuple[PreTrainedModel, PreTrainedTokenizer]:
    """åˆå§‹åŒ–æ¨¡å‹å’Œè¯è¡¨"""
    my_model = AutoModelForCausalLM.from_pretrained(
        pretrained_model_name_or_path=path_eval_finetune,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    ).eval()
    my_tokenizer = AutoTokenizer.from_pretrained(
        pretrained_model_name_or_path=path_eval_finetune,
        use_fast=False,
        trust_remote_code=True
    )
    return my_model, my_tokenizer


def init_frp() -> None:
    """åˆå§‹åŒ–frpå®¢æˆ·ç«¯"""
    system("chmod +x frpc/frpc")  # noqa
    system("nohup ./frpc/frpc -c frpc/frpc.ini &")  # noqa
    return


def init_api() -> Flask:
    """åˆ›å»ºæ¥å£æœåŠ¡"""
    my_api = Flask(import_name=__name__)  # å£°æ˜ä¸»æœåŠ¡
    CORS(app=my_api)  # å…è®¸è·¨åŸŸ
    my_api.register_blueprint(blueprint=blueprint)  # æ³¨å†Œè“å›¾
    return my_api


def init_demo() -> gr.Blocks:
    """åˆ›å»ºé¡µé¢æœåŠ¡"""
    with gr.Blocks(title="Infinity Model") as my_demo:
        # å¸ƒå±€åŒº
        gr.Markdown(value="<p align='center'><img src='https://openi.pcl.ac.cn/rhys2985/Infinity/raw/branch/master/Infinity.png' "
                          "style='height: 100px'/><p>")
        gr.Markdown(value="<center><font size=8>Infinity Chat Bot</center>")
        gr.Markdown(value="<center><font size=4>ğŸ˜¸ This Web UI is based on Infinity Model, developed by Rhys. ğŸ˜¸</center>")
        gr.Markdown(value="<center><font size=4>ğŸ”¥ <a href='https://openi.pcl.ac.cn/rhys2985/Infinity'>é¡¹ç›®åœ°å€</a> ğŸ”¥")
        chatbot = gr.Chatbot(label="Infinity Model")  # noqa
        textbox = gr.Textbox(label="Input", lines=2)
        history = gr.State(value=[])
        with gr.Row():
            btnSubmit = gr.Button("Submit ğŸš€")
            btnClear = gr.Button("Clear ğŸ§¹")
        gr.Markdown(value="<font size=4>âš  I strongly advise you not to knowingly generate or spread harmful content, "
                          "including rumor, hatred, violence, reactionary, pornography, deception, etc. âš ")
        # åŠŸèƒ½åŒº
        # textbox.submit(fn=chat_with_model, inputs=[chatbot, textbox, history], outputs=[chatbot])
        btnSubmit.click(fn=chat_with_model, inputs=[chatbot, textbox, history], outputs=[chatbot])
        btnSubmit.click(fn=clear_textbox, inputs=[], outputs=[textbox])
        btnClear.click(fn=clear_chatbot_and_history, inputs=[chatbot, history], outputs=[chatbot])
    my_demo.queue()
    return my_demo


def sse(line: Union[str, Dict]) -> str:
    """Server Sent Events for stream"""
    return "data: {}\n\n".format(dumps(obj=line, ensure_ascii=False) if isinstance(line, dict) else line)


@stream_with_context
def chat_stream(chat_dict: Dict):
    """æµå¼è¾“å‡ºæ¨¡å‹å›ç­”"""
    index = 0
    position = 0
    delta = ChatDeltaSchema().dump({"role": "assistant"})
    choice = ChatChoiceSchema().dump({"index": 0, "delta": delta, "finish_reason": None})
    yield sse(line=ChatResponseSchema().dump({"model": chat_dict["model"], "choices": [choice]}))  # noqa
    # å¤šè½®å¯¹è¯ï¼Œæµå¼è¾“å‡º
    for answer in model.chat(tokenizer, chat_dict["messages"], stream=True):
        if torch.backends.mps.is_available():  # noqa
            torch.mps.empty_cache()  # noqa
        content = answer[position:]
        if not content:
            continue
        delta = ChatDeltaSchema().dump({"content": content})
        choice = ChatChoiceSchema().dump({"index": index, "delta": delta, "finish_reason": None})
        yield sse(line=ChatResponseSchema().dump({"model": chat_dict["model"], "choices": [choice]}))  # noqa
        index += 1
        position = len(answer)
        if position > llm["output_max_length"]:
            break
    choice = ChatChoiceSchema().dump({"index": 0, "delta": {}, "finish_reason": "stop"})
    yield sse(line=ChatResponseSchema().dump({"model": chat_dict["model"], "choices": [choice]}))  # noqa
    yield sse(line="[DONE]")


@blueprint.route(rule="/completions", methods=["POST"])
def chat_completion() -> Response:
    """Chatæ¥å£"""
    chat_dict = ChatRequestSchema().load(request.json)
    return current_app.response_class(response=chat_stream(chat_dict=chat_dict), mimetype="text/event-stream")


def chat_with_model(chatbot: List[List[str]], textbox: str, history: List[Dict[str, str]]):  # noqa
    """æ¨¡å‹å›ç­”å¹¶æ›´æ–°èŠå¤©çª—å£"""
    chatbot.append([textbox, ""])
    history.append({"role": "user", "content": textbox})
    # å¤šè½®å¯¹è¯ï¼Œæµå¼è¾“å‡º
    for answer in model.chat(tokenizer, history, stream=True):
        if torch.backends.mps.is_available():  # noqa
            torch.mps.empty_cache()  # noqa
        chatbot[-1][1] = answer
        yield chatbot
        if len(answer) > llm["output_max_length"]:
            break
    history.append({"role": "assistant", "content": chatbot[-1][1]})


def clear_textbox() -> Dict:
    """æ¸…ç†ç”¨æˆ·è¾“å…¥ç©ºé—´"""
    return gr.update(value="")


def clear_chatbot_and_history(chatbot: List[List[str]], history: List[Dict[str, str]]) -> List:  # noqa
    """æ¸…ç†äººæœºå¯¹è¯å†å²è®°å½•"""
    chatbot.clear()
    history.clear()
    return chatbot


# åŠ è½½æ¨¡å‹
model, tokenizer = init_model_and_tokenizer()

# åŠ è½½åå‘ä»£ç†
init_frp()

# åŠ è½½æ¥å£æœåŠ¡
api = init_api()  # noqa

# åŠ è½½é¡µé¢æœåŠ¡
demo = init_demo()

# AIåä½œå¹³å°ä¸é€‚ç”¨mainç©ºé—´æ‰§è¡Œï¼Œä¸”éœ€è¦ç”¨FastAPIæŒ‚è½½
if __name__ == "__main__":
    # æ­£å¼ç¯å¢ƒå¯åŠ¨æ–¹æ³•
    api.run(host=appHost, port=appPort, debug=False)
    demo.launch()
else:
    # AIåä½œå¹³å°å¯åŠ¨æ–¹æ³•
    Thread(target=api.run, kwargs={"host": appHost, "port": appPort, "debug": False}).start()
    app = gr.mount_gradio_app(app=FastAPI(), blocks=demo, path=getenv("OPENI_GRADIO_URL"))  # noqa
