from json import dumps
from os import getenv, listdir
from re import match
from typing import Union, Optional, Tuple

import gradio as gr
from fastapi import FastAPI
from flask import Flask, Response, current_app, jsonify, render_template, request, stream_with_context
from flask_cors import CORS
from tiktoken import get_encoding

from utils import *


# STEP1.åŠ è½½æ¨¡å‹
# åŒ…æ‹¬å¯¹è¯æ¨¡å‹/è¡¥å…¨æ¨¡å‹å’ŒåµŒå…¥æ¨¡å‹ï¼Œå…¶ä¸­å¯¹è¯æ¨¡å‹/è¡¥å…¨æ¨¡å‹ä»pretrainmodelè·å–ï¼ŒåµŒå…¥æ¨¡å‹ä»datasetè·å–
# å¯¹è¯æ¨¡å‹/è¡¥å…¨æ¨¡å‹å¿…é€‰ï¼ŒåµŒå…¥æ¨¡å‹å¯é€‰
# pretrainmodelæ¨¡å‹ç±»åˆ«ä»model_type.txtæ–‡ä»¶ä¸­è·å–ï¼Œdatasetæ¨¡å‹ç±»åˆ«ä»å‹ç¼©æ–‡ä»¶åè·å–


def init_language_model() -> Union[BaseChatModel, BaseCompletionModel]:
    """åˆå§‹åŒ–æ¨¡å‹å’Œè¯è¡¨"""
    with open(file="{}/model_type.txt".format(path_eval_finetune), mode="r", encoding="utf-8") as f:
        my_model_name = f.read()
    if CHAT_MODEL_TYPE.get(my_model_name, None) is not None:
        my_model = CHAT_MODEL_TYPE[my_model_name](name=my_model_name, path=path_eval_finetune)
    elif COMPLETION_MODEL_TYPE.get(my_model_name, None) is not None:
        my_model = COMPLETION_MODEL_TYPE[my_model_name](name=my_model_name, path=path_eval_finetune)
    else:
        raise FileNotFoundError("no existing language model")
    return my_model


def init_embedding_model() -> Optional[BaseEmbeddingModel]:
    """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
    for filename in listdir(path_eval_pretrain):
        modelname = match(pattern="(.*)\.zip", string=filename)  # noqa
        if modelname is not None:
            my_model_name = modelname.groups()[0]
            break
    else:
        return None
    my_model = EMBEDDING_MODEL_TYPE[my_model_name](name=my_model_name, path=my_model_name)
    return my_model


language_model = init_language_model()
embedding_model = init_embedding_model()


# STEP2.å¯åŠ¨æ¥å£æœåŠ¡
# æ¥å£æœåŠ¡å‡é‡‡ç”¨çº¿ç¨‹å¯åŠ¨ï¼Œé¿å…é˜»å¡
# é‡‡ç”¨frpå·¥å…·åå‘ä»£ç†


def init_api() -> Flask:
    """åˆ›å»ºæ¥å£æœåŠ¡"""
    my_api = Flask(import_name=__name__)  # å£°æ˜ä¸»æœåŠ¡
    CORS(app=my_api)  # å…è®¸è·¨åŸŸ
    return my_api


api = init_api()
Thread(target=api.run, kwargs={"host": appHost, "port": appPort, "debug": False}).start()


@api.route(rule="/", methods=["GET"])
def homepage() -> str:
    """æ¥å£æœåŠ¡é¦–é¡µ"""
    return render_template(template_name_or_list="Infinity.html")  # noqa


@api.route(rule="/v1/chat/completions", methods=["POST"])
def chat() -> Response:
    """Chatæ¥å£"""
    req = ChatRequestSchema().load(request.json)
    if req["stream"]:
        # æµå¼å“åº”
        return current_app.response_class(response=chat_stream(req=req), mimetype="text/event-stream")
    else:
        # éæµå¼å“åº”
        return jsonify(chat_result(req=req))


# @api.route(rule="/v1/completions", methods=["POST"])
# def completions() -> Response:
#     """Completionsæ¥å£"""
#     return jsonify("")


@api.route(rule="/v1/embeddings", methods=["POST"])
def embeddings() -> Response:
    """Embeddingsæ¥å£"""
    req = EmbeddingsRequestSchema().load(request.json)
    return jsonify(embeddings_result(req=req))


def chat_result(req: Dict) -> str:
    """è¾“å‡ºæ¨¡å‹å›ç­”"""
    message = ChatMessageSchema().dump({"role": "assistant", "content": language_model.generate(conversation=req["messages"])})
    choice = ChatChoiceSchema().dump({"index": 0, "message": message})
    return ChatResponseSchema().dump({"model": language_model.name, "choices": [choice]})


@stream_with_context
def chat_stream(req: Dict):
    """æµå¼è¾“å‡ºæ¨¡å‹å›ç­”"""
    index = 0
    delta = ChatMessageSchema().dump({"role": "assistant", "content": ""})
    choice = ChatChoiceChunkSchema().dump({"index": index, "delta": delta, "finish_reason": None})
    yield chat_sse(line=ChatResponseChunkSchema().dump({"model": language_model.name, "choices": [choice]}))
    # å¤šè½®å¯¹è¯ï¼Œå­—ç¬¦å‹æµå¼è¾“å‡º
    for answer in language_model.stream(conversation=req["messages"]):
        index += 1
        delta = ChatMessageSchema().dump({"role": "assistant", "content": answer})
        choice = ChatChoiceChunkSchema().dump({"index": index, "delta": delta, "finish_reason": None})
        yield chat_sse(line=ChatResponseChunkSchema().dump({"model": language_model.name, "choices": [choice]}))
    choice = ChatChoiceChunkSchema().dump({"index": 0, "delta": {}, "finish_reason": "stop"})
    yield chat_sse(line=ChatResponseChunkSchema().dump({"model": language_model.name, "choices": [choice]}))
    yield chat_sse(line="[DONE]")


def chat_sse(line: Union[str, Dict]) -> str:
    """Server Sent Events for stream"""
    return "data: {}\n\n".format(dumps(obj=line, ensure_ascii=False) if isinstance(line, dict) else line)


def embeddings_result(req: Dict) -> str:
    """è®¡ç®—åµŒå…¥ç»“æœ"""
    data = [{"index": index, "embedding": embedding_model.embedding(sentence=text) if embedding_model is not None else []}
            for index, text in enumerate(req["input"])]
    usage = {
        "prompt_tokens": sum(len(text.split()) for text in req["input"]),
        "total_tokens": sum(embeddings_token_num(text=text) for text in req["input"])
    }
    return EmbeddingsResponseSchema().dump({
        "model": embedding_model.name if embedding_model is not None else "", "data": data, "usage": usage})


def embeddings_token_num(text: str) -> int:
    """è®¡ç®—åµŒå…¥æ¶ˆè€—"""
    return len(get_encoding(encoding_name="cl100k_base").encode(text=text))


# STEP3.å¯åŠ¨é¡µé¢æœåŠ¡
# AIåä½œå¹³å°ä¸é€‚ç”¨mainç©ºé—´æ‰§è¡Œï¼Œä¸”éœ€è¦ç”¨FastAPIæŒ‚è½½


def submit(chatbot: List[List[str]], textbox: str, history: List[Dict[str, str]]) -> Tuple[List[List[str]], str]:  # noqa
    """æ¨¡å‹å›ç­”å¹¶æ›´æ–°èŠå¤©çª—å£"""
    history.append({"role": "user", "content": textbox})
    answer = language_model.generate(conversation=history)  # å¤šè½®å¯¹è¯ï¼Œéæµå¼æ–‡æœ¬è¾“å‡º
    history.append({"role": "assistant", "content": answer})
    chatbot.append([textbox, answer])
    return chatbot, ""


def clean(chatbot: List[List[str]], history: List[Dict[str, str]]) -> List[List[str]]:  # noqa
    """æ¸…ç†äººæœºå¯¹è¯å†å²è®°å½•"""
    chatbot.clear()
    history.clear()
    return chatbot


def init_demo() -> gr.Blocks:
    """åˆ›å»ºé¡µé¢æœåŠ¡"""
    with gr.Blocks(title="Infinity Model") as my_demo:
        # å¸ƒå±€åŒº
        gr.Markdown(value="<p align='center'>"
                          "<img src='https://openi.pcl.ac.cn/rhys2985/Infinity/raw/branch/master/templates/Infinity.png' "
                          "style='height: 100px'>"
                          "</p>")
        gr.Markdown(value="<center><font size=8>Infinity Chat Bot</center>")
        gr.Markdown(value="<center><font size=4>ğŸ˜¸ This Web UI is based on Infinity Model, developed by Rhys. ğŸ˜¸</center>")
        gr.Markdown(value="<center><font size=4>ğŸ”¥ <a href='https://openi.pcl.ac.cn/rhys2985/Infinity'>é¡¹ç›®åœ°å€</a> ğŸ”¥</center>")
        with gr.Row():
            with gr.Column(scale=1):
                pass
            with gr.Column(scale=5):
                chatbot = gr.Chatbot(label="Infinity Model")  # noqa
                textbox = gr.Textbox(label="Input", lines=2)
                history = gr.State(value=[])
                with gr.Row():
                    btnSubmit = gr.Button("Submit ğŸš€")
                    btnClean = gr.Button("Clean ğŸ§¹")
            with gr.Column(scale=1):
                pass
        gr.Markdown(value="<center><font size=4>âš  I strongly advise you not to knowingly generate or spread harmful content, "
                          "including rumor, hatred, violence, reactionary, pornography, deception, etc. âš </center>")
        # åŠŸèƒ½åŒº
        btnSubmit.click(fn=submit, inputs=[chatbot, textbox, history], outputs=[chatbot, textbox])
        btnClean.click(fn=clean, inputs=[chatbot, history], outputs=[chatbot])
    return my_demo


demo = init_demo()
# æ­£å¼ç¯å¢ƒå¯åŠ¨æ–¹æ³•
if __name__ == "__main__":
    demo.launch()
# AIåä½œå¹³å°å¯åŠ¨æ–¹æ³•
else:
    app = gr.mount_gradio_app(app=FastAPI(), blocks=demo, path=getenv("OPENI_GRADIO_URL"))  # noqa
