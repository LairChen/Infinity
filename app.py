from json import dumps
from os import getenv, listdir, system
from re import match
from typing import Union, Optional

import gradio as gr
from fastapi import FastAPI
from flask import Flask, Response, current_app, jsonify, render_template, request, stream_with_context
from flask_cors import CORS
from tiktoken import get_encoding

from utils import *


# STEP1.åŠ è½½æ¨¡å‹
# åŒ…æ‹¬å¯¹è¯æ¨¡å‹å’ŒåµŒå…¥æ¨¡å‹ï¼Œå…¶ä¸­å¯¹è¯æ¨¡å‹ä»pretrainmodelè·å–ï¼ŒåµŒå…¥æ¨¡å‹ä»datasetè·å–
# å¯¹è¯æ¨¡å‹å¿…é¡»ï¼ŒåµŒå…¥æ¨¡å‹å¯é€‰
# modelæ¨¡å‹ç±»åˆ«ä»model_type.txtæ–‡ä»¶ä¸­è·å–ï¼Œdatasetæ¨¡å‹ç±»åˆ«ä»å‹ç¼©æ–‡ä»¶åè·å–


def init_chat_model() -> BaseChatModel:
    """åˆå§‹åŒ–æ¨¡å‹å’Œè¯è¡¨"""
    with open(file="{}/model_type.txt".format(path_eval_finetune), mode="r", encoding="utf-8") as f:
        my_model_name = f.read()
    my_model = CHAT_MODEL_TYPE[my_model_name](name=my_model_name, path=path_eval_finetune)
    return my_model


def init_embeddings_model() -> Optional[BaseEmbeddingsModel]:
    """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
    for filename in listdir(path_eval_pretrain):
        modelname = match(pattern="(.*)\.zip", string=filename)  # noqa
        if modelname is not None:
            my_model_name = modelname.groups()[0]
            break
    else:
        return None
    my_model = EMBEDDINGS_MODEL_TYPE[my_model_name](name=my_model_name, path=my_model_name)
    return my_model


chat_model = init_chat_model()
embeddings_model = init_embeddings_model()


# STEP2.å¯åŠ¨æ¥å£æœåŠ¡
# æ¥å£æœåŠ¡å‡é‡‡ç”¨çº¿ç¨‹å¯åŠ¨ï¼Œé¿å…é˜»å¡
# é‡‡ç”¨frpå·¥å…·åå‘ä»£ç†


def init_api() -> Flask:
    """åˆ›å»ºæ¥å£æœåŠ¡"""
    system("chmod +x frpc/frpc")  # noqa
    system("nohup ./frpc/frpc -c frpc/frpc.ini &")  # noqa
    my_api = Flask(import_name=__name__)  # å£°æ˜ä¸»æœåŠ¡
    CORS(app=my_api)  # å…è®¸è·¨åŸŸ
    return my_api


api = init_api()
Thread(target=api.run, kwargs={"host": appHost, "port": appPort, "debug": False}).start()


@api.route(rule="/", methods=["GET"])
def homepage() -> str:
    """æ¥å£æœåŠ¡é¦–é¡µ"""
    return render_template(template_name_or_list="Infinity.html")  # noqa


@api.route(rule="/v1/chat/completions", methods=["POST"])
def chat() -> Response:
    """Chatæ¥å£"""
    req = ChatRequestSchema().load(request.json)
    return current_app.response_class(response=chat_result(req=req), mimetype="text/event-stream")


@api.route(rule="/v1/embeddings", methods=["POST"])
def embeddings() -> Response:
    """Embeddingsæ¥å£"""
    req = EmbeddingsRequestSchema().load(request.json)
    return jsonify(embeddings_result(req=req))


@stream_with_context
def chat_result(req: Dict):
    """æµå¼è¾“å‡ºæ¨¡å‹å›ç­”"""
    index = 0
    delta = ChatDeltaSchema().dump({"role": "assistant"})
    choice = ChatChoiceSchema().dump({"index": 0, "delta": delta, "finish_reason": None})
    yield chat_sse(line=ChatResponseSchema().dump({"model": chat_model.name, "choices": [choice]}))  # noqa
    # å¤šè½®å¯¹è¯ï¼Œå­—ç¬¦å‹æµå¼è¾“å‡º
    for answer in chat_model.stream(conversation=req["messages"]):
        delta = ChatDeltaSchema().dump({"content": answer})
        choice = ChatChoiceSchema().dump({"index": index, "delta": delta, "finish_reason": None})
        yield chat_sse(line=ChatResponseSchema().dump({"model": chat_model.name, "choices": [choice]}))  # noqa
        index += 1
    choice = ChatChoiceSchema().dump({"index": 0, "delta": {}, "finish_reason": "stop"})
    yield chat_sse(line=ChatResponseSchema().dump({"model": chat_model.name, "choices": [choice]}))  # noqa
    yield chat_sse(line="[DONE]")


def chat_sse(line: Union[str, Dict]) -> str:
    """Server Sent Events for stream"""
    return "data: {}\n\n".format(dumps(obj=line, ensure_ascii=False) if isinstance(line, dict) else line)


def embeddings_result(req: Dict) -> Dict:
    """è®¡ç®—åµŒå…¥ç»“æœ"""
    data = [{"index": index, "embedding": [] if embeddings_model is None else embeddings_model.embedding(sentence=text)}
            for index, text in req["input"]]
    usage = {
        "prompt_tokens": sum(len(text.split()) for text in req["input"]),
        "total_tokens": sum(embeddings_token_num(text=text) for text in req["input"])
    }
    return EmbeddingsResponseSchema().dump({"model": req["model"], "data": data, "usage": usage})


def embeddings_token_num(text: str) -> int:
    """è®¡ç®—åµŒå…¥æ¶ˆè€—"""
    return len(get_encoding(encoding_name="cl100k_base").encode(text=text))


# STEP3.å¯åŠ¨é¡µé¢æœåŠ¡
# AIåä½œå¹³å°ä¸é€‚ç”¨mainç©ºé—´æ‰§è¡Œï¼Œä¸”éœ€è¦ç”¨FastAPIæŒ‚è½½


def refresh_chatbot_and_history(chatbot: List[List[str]], textbox: str, history: List[Dict[str, str]]) -> List[List[str]]:  # noqa
    """æ¨¡å‹å›ç­”å¹¶æ›´æ–°èŠå¤©çª—å£"""
    answer = chat_model.generate(conversation=history)
    # å¤šè½®å¯¹è¯ï¼Œæ–‡æœ¬è¾“å‡º
    chatbot.append([textbox, answer])
    history.append({"role": "user", "content": textbox})
    history.append({"role": "assistant", "content": answer})
    return chatbot


def clear_textbox() -> Dict:
    """æ¸…ç†ç”¨æˆ·è¾“å…¥ç©ºé—´"""
    return gr.update(value="")


def clear_chatbot_and_history(chatbot: List[List[str]], history: List[Dict[str, str]]) -> List:  # noqa
    """æ¸…ç†äººæœºå¯¹è¯å†å²è®°å½•"""
    chatbot.clear()
    history.clear()
    return chatbot


def init_demo() -> gr.Blocks:
    """åˆ›å»ºé¡µé¢æœåŠ¡"""
    with gr.Blocks(title="Infinity Model") as my_demo:
        # å¸ƒå±€åŒº
        gr.Markdown(value="<p align='center'>"
                          "<img src='https://openi.pcl.ac.cn/rhys2985/Infinity/raw/branch/master/templates/Infinity.png' "
                          "style='height: 100px'>"
                          "</p>")
        gr.Markdown(value="<center><font size=8>Infinity Chat Bot</center>")
        gr.Markdown(value="<center><font size=4>ğŸ˜¸ This Web UI is based on Infinity Model, developed by Rhys. ğŸ˜¸</center>")
        gr.Markdown(value="<center><font size=4>ğŸ”¥ <a href='https://openi.pcl.ac.cn/rhys2985/Infinity'>é¡¹ç›®åœ°å€</a> ğŸ”¥</center>")
        chatbot = gr.Chatbot(label="Infinity Model")  # noqa
        textbox = gr.Textbox(label="Input", lines=2)
        history = gr.State(value=[])
        with gr.Row():
            btnSubmit = gr.Button("Submit ğŸš€")
            btnClean = gr.Button("Clean ğŸ§¹")
        gr.Markdown(value="<center><font size=4>âš  I strongly advise you not to knowingly generate or spread harmful content, "
                          "including rumor, hatred, violence, reactionary, pornography, deception, etc. âš </center>")
        # åŠŸèƒ½åŒº
        btnSubmit.click(fn=refresh_chatbot_and_history, inputs=[chatbot, textbox, history], outputs=[chatbot])
        btnSubmit.click(fn=clear_textbox, inputs=[], outputs=[textbox])
        btnClean.click(fn=clear_chatbot_and_history, inputs=[chatbot, history], outputs=[chatbot])
    return my_demo


demo = init_demo()
# æ­£å¼ç¯å¢ƒå¯åŠ¨æ–¹æ³•
if __name__ == "__main__":
    demo.launch()
# AIåä½œå¹³å°å¯åŠ¨æ–¹æ³•
else:
    app = gr.mount_gradio_app(app=FastAPI(), blocks=demo, path=getenv("OPENI_GRADIO_URL"))  # noqa
