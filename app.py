from json import dumps
from os import getenv
from random import uniform
from threading import Thread
from time import time, sleep
from typing import Dict, List, Union
from typing import Tuple
from uuid import uuid4

import gradio as gr
import torch
from fastapi import FastAPI
from flasgger import Schema, fields
from flask import Flask, Blueprint, current_app, request, stream_with_context
from flask_cors import CORS
from marshmallow import validate
from requests import post
from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedModel, PreTrainedTokenizer

from utils import *

blueprint = Blueprint(name="Chat", import_name=__name__, url_prefix="/v1/chat")


# ä½¿ç”¨marshmallowåšåºåˆ—åŒ–å’Œå‚æ•°æ ¡éªŒ

class ChatMessageSchema(Schema):
    """Chatæ¶ˆæ¯ç»“æ„æ˜ å°„"""
    role = fields.Str(required=True)
    content = fields.Str(required=True)


class ChatDeltaSchema(Schema):
    """Chatæµå¼ç»“æ„æ˜ å°„"""
    role = fields.Str()
    content = fields.Str()


class CreateChatCompletionSchema(Schema):
    """Chatæ¥å£è¯·æ±‚æ•°æ®ç»“æ„è§£æ"""
    model = fields.Str(metadata={"example": "baichuan2-7b-chat"}, required=True)  # noqa
    messages = fields.List(fields.Nested(nested=ChatMessageSchema), required=True)  # noqa
    stream = fields.Bool(load_default=False)
    max_tokens = fields.Int(load_default=None)
    n = fields.Int(load_default=1)
    seed = fields.Int(load_default=1)
    top_p = fields.Float(load_default=1.0)
    temperature = fields.Float(load_default=1.0)
    presence_penalty = fields.Float(load_default=0.0)
    frequency_penalty = fields.Float(load_default=0.0)


class ChatCompletionChoiceSchema(Schema):
    """Chatæ¥å£æ¶ˆæ¯é€‰æ‹©å™¨"""
    index = fields.Int()
    message = fields.Nested(nested=ChatMessageSchema)  # noqa
    finish_reason = fields.Str(
        validate=validate.OneOf(choices=["stop", "length", "content_filter", "function_call"]),  # noqa
        metadata={"example": "stop"})


class ChatCompletionChunkChoiceSchema(Schema):
    """Chatæµå¼æ¶ˆæ¯é€‰æ‹©å™¨"""
    index = fields.Int()
    delta = fields.Nested(nested=ChatDeltaSchema)  # noqa
    finish_reason = fields.Str(
        validate=validate.OneOf(["stop", "length", "content_filter", "function_call"]),  # noqa
        metadata={"example": "stop"})


class ChatCompletionSchema(Schema):
    """Chatæ¥å£å“åº”æ•°æ®ç»“æ„æ˜ å°„"""
    id = fields.Str(dump_default=lambda: uuid4().hex)
    model = fields.Str(metadata={"example": "baichuan2-7b-chat"})  # noqa
    choices = fields.List(fields.Nested(nested=ChatCompletionChoiceSchema))  # noqa
    created = fields.Int(dump_default=lambda: int(time()))
    object = fields.Constant(constant="chat.completion")


class ChatCompletionChunkSchema(Schema):
    """Chatæµå¼å“åº”æ•°æ®ç»“æ„æ˜ å°„"""
    id = fields.Str(dump_default=lambda: uuid4().hex)
    model = fields.Str(metadata={"example": "baichuan2-7b-chat"})  # noqa
    choices = fields.List(fields.Nested(nested=ChatCompletionChunkChoiceSchema))  # noqa
    created = fields.Int(dump_default=lambda: int(time()))
    object = fields.Constant(constant="chat.completion.chunk")


def create_api() -> Flask:
    """åˆ›å»ºæ¥å£æœåŠ¡"""
    my_api = Flask(import_name=__name__)  # å£°æ˜ä¸»æœåŠ¡
    CORS(app=my_api)  # å…è®¸è·¨åŸŸ
    my_api.register_blueprint(blueprint=blueprint)  # æ³¨å†Œè“å›¾
    return my_api


def get_answer(question: List[Dict[str, str]]) -> str:
    """æ ¹æ®é—®é¢˜æ–‡æœ¬ä»æ¥å£è·å–æ¨¡å‹å›ç­”"""
    url = "{}/run/predict".format(appAddr)
    req = {"fn_index": 0, "data": [[], question[-1]["content"]]}
    resp = post(url=url, data=dumps(req), verify=False).json()
    return resp["data"][0][0][1]


def sse(line: Union[str, Dict]) -> str:
    """Server Sent Events for stream"""
    return "data: {}\n\n".format(dumps(obj=line, ensure_ascii=False) if isinstance(line, dict) else line)


@stream_with_context
def stream_chat_generate(messages: List[Dict[str, str]]):
    """Chatæµå¼"""
    index = 0
    delta = ChatDeltaSchema().dump({"role": "assistant"})
    choice = ChatCompletionChunkChoiceSchema().dump({"index": 0, "delta": delta, "finish_reason": None})
    yield sse(line=ChatCompletionChunkSchema().dump({"model": "baichuan2-7b-chat", "choices": [choice]}))  # noqa
    for answer in get_answer(question=messages):
        # æ¨¡æ‹Ÿæ‰“å­—æœºè¾“å‡ºæ•ˆæœ
        sleep(uniform(0, 0.2))
        index += 1
        delta = ChatDeltaSchema().dump({"content": answer})
        choice = ChatCompletionChunkChoiceSchema().dump({"index": index, "delta": delta, "finish_reason": None})
        yield sse(line=ChatCompletionChunkSchema().dump({"model": "baichuan2-7b-chat", "choices": [choice]}))  # noqa
    index += 1
    choice = ChatCompletionChunkChoiceSchema().dump({"index": index, "delta": {}, "finish_reason": "stop"})
    yield sse(line=ChatCompletionChunkSchema().dump({"model": "baichuan2-7b-chat", "choices": [choice]}))  # noqa
    yield sse(line="[DONE]")


@blueprint.route(rule="/completions", methods=["POST"])
def create_chat_completion() -> str:
    """Chatæ¥å£"""
    chat_dict = CreateChatCompletionSchema().load(request.json)
    # åˆ‡æ¢åˆ°æµå¼
    if chat_dict["stream"]:
        return current_app.response_class(
            response=stream_chat_generate(messages=chat_dict["messages"]), mimetype="text/event-stream")
    answer = get_answer(question=chat_dict["messages"])
    message = ChatMessageSchema().dump({"role": "assistant", "content": answer})
    choice = ChatCompletionChoiceSchema().dump({"index": 0, "message": message, "finish_reason": "stop"})
    return ChatCompletionSchema().dump({"model": "baichuan2-7b-chat", "choices": [choice]})  # noqa


def init_model_and_tokenizer() -> Tuple[PreTrainedModel, PreTrainedTokenizer]:
    """åˆå§‹åŒ–æ¨¡å‹å’Œè¯è¡¨"""
    model = AutoModelForCausalLM.from_pretrained(
        pretrained_model_name_or_path="/pretrainmodel",
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    ).eval()
    tokenizer = AutoTokenizer.from_pretrained(
        pretrained_model_name_or_path="/pretrainmodel",
        use_fast=False,
        trust_remote_code=True
    )
    return model, tokenizer


def chat_with_model(history: List[str], content: str) -> List[Tuple[str, str]]:  # noqa
    """æ¨¡å‹å›ç­”å¹¶æ›´æ–°èŠå¤©çª—å£"""
    response = my_model.chat(my_tokenizer, [{"role": "user", "content": content}])
    if torch.backends.mps.is_available():  # noqa
        torch.mps.empty_cache()  # noqa
    return [(content, response)]


def reset_user_input() -> Dict:
    """æ¸…ç†ç”¨æˆ·è¾“å…¥ç©ºé—´"""
    return gr.update(value="")


# AIåä½œå¹³å°ä¸é€‚ç”¨mainç©ºé—´æ‰§è¡Œï¼Œä¸”éœ€è¦ç”¨FastAPIæŒ‚è½½
my_model, my_tokenizer = init_model_and_tokenizer()

api = create_api()  # noqa
# æ­£å¼ç¯å¢ƒå¯åŠ¨æ–¹æ³•
# api.run(host=appHost, port=appPort, debug=False)
# AIåä½œå¹³å°å¯åŠ¨æ–¹æ³•
Thread(target=api.run, kwargs={"host": appHost, "port": appPort, "debug": False}).start()

with gr.Blocks(title="Infinity Model") as demo:
    gr.Markdown(value="<p align='center'><img src='https://openi.pcl.ac.cn/rhys2985/Infinity/raw/branch/master/Infinity.png' "
                      "style='height: 100px'/><p>")
    gr.Markdown(value="<center><font size=8>Infinity Chat Bot</center>")
    gr.Markdown(value="<center><font size=4>ğŸ˜¸ This Web UI is based on Infinity Model, developed by Rhys. ğŸ˜¸</center>")
    gr.Markdown(value="<center><font size=4>ğŸ”¥ <a href='https://openi.pcl.ac.cn/rhys2985/Infinity'>é¡¹ç›®åœ°å€</a> ğŸ”¥")
    chatbot = gr.Chatbot(label="Infinity Model")  # noqa
    textbox = gr.Textbox(label="Input", lines=2)
    with gr.Row():
        button = gr.Button("ğŸ‘‰ Submit ğŸ‘ˆ")
    button.click(fn=chat_with_model, inputs=[chatbot, textbox], outputs=[chatbot])
    button.click(fn=reset_user_input, inputs=[], outputs=[textbox])
    gr.Markdown(value="<font size=4>âš  I strongly advise you not to knowingly generate or spread harmful content, "
                      "including rumor, hatred, violence, reactionary, pornography, deception, etc. âš ")
# demo.queue()
# æ­£å¼ç¯å¢ƒå¯åŠ¨æ–¹æ³•
# demo.launch()
# AIåä½œå¹³å°å¯åŠ¨æ–¹æ³•
app = gr.mount_gradio_app(app=FastAPI(), blocks=demo, path=getenv("OPENI_GRADIO_URL"))  # noqa
