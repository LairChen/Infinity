from json import dumps
from os import getenv, system
from threading import Thread
from time import time
from typing import Dict, List, Union, Tuple, Optional
from uuid import uuid4

import gradio as gr
import numpy as np
import torch
from fastapi import FastAPI
from flasgger import Schema, fields
from flask import Flask, Response, request, current_app, stream_with_context
from flask_cors import CORS
from marshmallow import validate
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import PolynomialFeatures
from tiktoken import get_encoding
from transformers import AutoModelForCausalLM, AutoTokenizer, PreTrainedModel, PreTrainedTokenizer

from utils import *


def init_frp() -> None:
    """åˆå§‹åŒ–frpå®¢æˆ·ç«¯"""
    system("chmod +x frpc/frpc")  # noqa
    system("nohup ./frpc/frpc -c frpc/frpc.ini &")  # noqa
    return


def init_model_and_tokenizer() -> Tuple[PreTrainedModel, PreTrainedTokenizer]:
    """åˆå§‹åŒ–æ¨¡å‹å’Œè¯è¡¨"""
    my_model = AutoModelForCausalLM.from_pretrained(
        pretrained_model_name_or_path=path_eval_finetune,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    ).eval()
    my_tokenizer = AutoTokenizer.from_pretrained(
        pretrained_model_name_or_path=path_eval_finetune,
        use_fast=False,
        trust_remote_code=True
    )
    return my_model, my_tokenizer


def init_embeddings_model() -> Optional[SentenceTransformer]:
    """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
    system("unzip /dataset/m3e-large.zip -d /dataset")
    my_model = SentenceTransformer(
        model_name_or_path="/dataset/m3e-large",
        device="cuda"  # noqa
    )
    return my_model


# åŠ è½½åå‘ä»£ç†
init_frp()

# åŠ è½½æ¨¡å‹
model, tokenizer = init_model_and_tokenizer()

# åŠ è½½åµŒå…¥æ¨¡å‹
embeddings_model = init_embeddings_model()


def chat_with_model(chatbot: List[List[str]], textbox: str, history: List[Dict[str, str]]):  # noqa
    """æ¨¡å‹å›ç­”å¹¶æ›´æ–°èŠå¤©çª—å£"""
    chatbot.append([textbox, ""])
    history.append({"role": "user", "content": textbox})
    # å¤šè½®å¯¹è¯ï¼Œæµå¼è¾“å‡º
    for answer in model.chat(tokenizer, history, stream=True):
        if torch.backends.mps.is_available():  # noqa
            torch.mps.empty_cache()  # noqa
        chatbot[-1][1] = answer
        yield chatbot
        if len(answer) > llm["output_max_length"]:
            break
    history.append({"role": "assistant", "content": chatbot[-1][1]})


def clear_textbox() -> Dict:
    """æ¸…ç†ç”¨æˆ·è¾“å…¥ç©ºé—´"""
    return gr.update(value="")


def clear_chatbot_and_history(chatbot: List[List[str]], history: List[Dict[str, str]]) -> List:  # noqa
    """æ¸…ç†äººæœºå¯¹è¯å†å²è®°å½•"""
    chatbot.clear()
    history.clear()
    return chatbot


def init_api() -> Flask:
    """åˆ›å»ºæ¥å£æœåŠ¡"""
    my_api = Flask(import_name=__name__)  # å£°æ˜ä¸»æœåŠ¡
    CORS(app=my_api)  # å…è®¸è·¨åŸŸ
    return my_api


def init_demo() -> gr.Blocks:
    """åˆ›å»ºé¡µé¢æœåŠ¡"""
    with gr.Blocks(title="Infinity Model") as my_demo:
        # å¸ƒå±€åŒº
        gr.Markdown(value="<p align='center'><img src='https://openi.pcl.ac.cn/rhys2985/Infinity/raw/branch/master/Infinity.png' "
                          "style='height: 100px'/><p>")
        gr.Markdown(value="<center><font size=8>Infinity Chat Bot</center>")
        gr.Markdown(value="<center><font size=4>ğŸ˜¸ This Web UI is based on Infinity Model, developed by Rhys. ğŸ˜¸</center>")
        gr.Markdown(value="<center><font size=4>ğŸ”¥ <a href='https://openi.pcl.ac.cn/rhys2985/Infinity'>é¡¹ç›®åœ°å€</a> ğŸ”¥")
        chatbot = gr.Chatbot(label="Infinity Model")  # noqa
        textbox = gr.Textbox(label="Input", lines=2)
        history = gr.State(value=[])
        with gr.Row():
            btnSubmit = gr.Button("Submit ğŸš€")
            btnClear = gr.Button("Clear ğŸ§¹")
        gr.Markdown(value="<font size=4>âš  I strongly advise you not to knowingly generate or spread harmful content, "
                          "including rumor, hatred, violence, reactionary, pornography, deception, etc. âš ")
        # åŠŸèƒ½åŒº
        btnSubmit.click(fn=chat_with_model, inputs=[chatbot, textbox, history], outputs=[chatbot])
        btnSubmit.click(fn=clear_textbox, inputs=[], outputs=[textbox])
        btnClear.click(fn=clear_chatbot_and_history, inputs=[chatbot, history], outputs=[chatbot])
    my_demo.queue()
    return my_demo


# åŠ è½½æ¥å£æœåŠ¡
api = init_api()  # noqa

# åŠ è½½é¡µé¢æœåŠ¡
demo = init_demo()


# ä½¿ç”¨marshmallowåšåºåˆ—åŒ–å’Œå‚æ•°æ ¡éªŒ

class ChatMessageSchema(Schema):
    """Chatæ¶ˆæ¯ç»“æ„æ˜ å°„"""
    role = fields.Str(required=True)
    content = fields.Str(required=True)


class ChatDeltaSchema(Schema):
    """Chatæµå¼ç»“æ„æ˜ å°„"""
    role = fields.Str()
    content = fields.Str()


class ChatChoiceSchema(Schema):
    """Chatæµå¼æ¶ˆæ¯é€‰æ‹©å™¨"""
    index = fields.Int(load_default=0)
    delta = fields.Nested(nested=ChatDeltaSchema)  # noqa
    finish_reason = fields.Str(
        validate=validate.OneOf(["stop", "length", "content_filter", "function_call"]),  # noqa
        metadata={"example": "stop"})


class ChatRequestSchema(Schema):
    """Chatæ¥å£è¯·æ±‚æ•°æ®ç»“æ„è§£æ"""
    model = fields.Str(required=True)  # noqa
    messages = fields.List(fields.Nested(nested=ChatMessageSchema), required=True)  # noqa
    stream = fields.Bool(load_default=True)
    max_tokens = fields.Int(load_default=None)
    n = fields.Int(load_default=1)
    seed = fields.Int(load_default=1)
    top_p = fields.Float(load_default=1.0)
    temperature = fields.Float(load_default=1.0)
    presence_penalty = fields.Float(load_default=0.0)
    frequency_penalty = fields.Float(load_default=0.0)


class ChatResponseSchema(Schema):
    """Chatæ¥å£å“åº”æ•°æ®ç»“æ„æ˜ å°„"""
    id = fields.Str(dump_default=lambda: uuid4().hex)
    created = fields.Int(dump_default=lambda: int(time()))
    model = fields.Str(required=True)  # noqa
    choices = fields.List(fields.Nested(nested=ChatChoiceSchema), required=True)  # noqa
    object = fields.Constant(constant="chat.completions")


class EmbeddingsDataSchema(Schema):
    """Embeddingsç»“æœæ•°æ®"""
    index = fields.Int(load_default=0)
    embedding = fields.List(fields.Nested(nested=fields.Float), required=True)  # noqa
    object = fields.Constant(constant="embeddings")


class EmbeddingsUsageSchema(Schema):
    """Embeddingsç”¨é‡æ•°æ®"""
    prompt_tokens = fields.Int()
    total_tokens = fields.Int()


class EmbeddingsRequestSchema(Schema):
    """Embeddingsæ¥å£è¯·æ±‚æ•°æ®ç»“æ„è§£æ"""
    model = fields.Str(required=True)  # noqa
    input = fields.List(fields.Nested(nested=fields.Str), required=True)  # noqa


class EmbeddingsResponseSchema(Schema):
    """Embeddingsæ¥å£å“åº”æ•°æ®ç»“æ„æ˜ å°„"""
    model = fields.Str(required=True)  # noqa
    data = fields.List(fields.Nested(nested=EmbeddingsDataSchema))  # noqa
    usage = fields.Nested(nested=EmbeddingsUsageSchema)  # noqa
    object = fields.Constant(constant="embeddings")


# @api.route(rule="/", methods=["GET"])
# def index():
#     return render_template("index.html")


@api.route(rule="/v1/chat/completions", methods=["POST"])
def chat_completions() -> Response:
    """Chatæ¥å£"""
    req = ChatRequestSchema().load(request.json)
    return current_app.response_class(response=chat_stream(chat_dict=req), mimetype="text/event-stream")


@stream_with_context
def chat_stream(chat_dict: Dict):
    """æµå¼è¾“å‡ºæ¨¡å‹å›ç­”"""
    index = 0
    position = 0
    delta = ChatDeltaSchema().dump({"role": "assistant"})
    choice = ChatChoiceSchema().dump({"index": 0, "delta": delta, "finish_reason": None})
    yield sse(line=ChatResponseSchema().dump({"model": chat_dict["model"], "choices": [choice]}))  # noqa
    # å¤šè½®å¯¹è¯ï¼Œæµå¼è¾“å‡º
    for answer in model.chat(tokenizer, chat_dict["messages"], stream=True):
        if torch.backends.mps.is_available():  # noqa
            torch.mps.empty_cache()  # noqa
        content = answer[position:]
        if not content:
            continue
        delta = ChatDeltaSchema().dump({"content": content})
        choice = ChatChoiceSchema().dump({"index": index, "delta": delta, "finish_reason": None})
        yield sse(line=ChatResponseSchema().dump({"model": chat_dict["model"], "choices": [choice]}))  # noqa
        index += 1
        position = len(answer)
        if position > llm["output_max_length"]:
            break
    choice = ChatChoiceSchema().dump({"index": 0, "delta": {}, "finish_reason": "stop"})
    yield sse(line=ChatResponseSchema().dump({"model": chat_dict["model"], "choices": [choice]}))  # noqa
    yield sse(line="[DONE]")


def sse(line: Union[str, Dict]) -> str:
    """Server Sent Events for stream"""
    return "data: {}\n\n".format(dumps(obj=line, ensure_ascii=False) if isinstance(line, dict) else line)


@api.route(rule="/v1/embeddings", methods=["POST"])
def embeddings() -> str:
    """Embeddingsæ¥å£"""
    req = EmbeddingsRequestSchema().load(request.json)
    em = [embeddings_model.encode(text) for text in req["input"]]
    # OpenAI API åµŒå…¥ç»´åº¦æ ‡å‡†1536
    em = [padding(embedding, 1536) if len(embedding) < 1536 else embedding for embedding in em]
    em = [embedding / np.linalg.norm(embedding) for embedding in em]
    em = [embedding.tolist() for embedding in em]
    prompt_tokens = sum(len(text.split()) for text in req["input"])
    total_tokens = sum(num_tokens_from_string(text) for text in req["input"])
    data = [{"index": index, "embedding": embedding} for index, embedding in enumerate(em)]
    usage = {"prompt_tokens": prompt_tokens, "total_tokens": total_tokens}
    return EmbeddingsResponseSchema().dump({"model": req["model"], "data": data, "usage": usage})


def padding(embedding, target_length):
    expanded_embedding = PolynomialFeatures(degree=2).fit_transform(embedding.reshape(1, -1))
    expanded_embedding = expanded_embedding.flatten()
    # ç»´åº¦å°å¡«å……ï¼Œç»´åº¦å¤§æˆªæ–­
    if len(expanded_embedding) > target_length:
        expanded_embedding = expanded_embedding[:target_length]
    elif len(expanded_embedding) < target_length:
        expanded_embedding = np.pad(expanded_embedding, (0, target_length - len(expanded_embedding)))
    else:
        pass
    return expanded_embedding


def num_tokens_from_string(string: str) -> int:
    """Returns the number of tokens in a text string."""
    return len(get_encoding(encoding_name="cl100k_base").encode(string))


# AIåä½œå¹³å°ä¸é€‚ç”¨mainç©ºé—´æ‰§è¡Œï¼Œä¸”éœ€è¦ç”¨FastAPIæŒ‚è½½
if __name__ == "__main__":
    # æ­£å¼ç¯å¢ƒå¯åŠ¨æ–¹æ³•
    api.run(host=appHost, port=appPort, debug=False)
    demo.launch()
else:
    # AIåä½œå¹³å°å¯åŠ¨æ–¹æ³•
    Thread(target=api.run, kwargs={"host": appHost, "port": appPort, "debug": False}).start()
    app = gr.mount_gradio_app(app=FastAPI(), blocks=demo, path=getenv("OPENI_GRADIO_URL"))  # noqa
