from json import dumps
from os import getenv, listdir, system
from threading import Thread
from typing import Dict, List, Union, Tuple, Optional

import gradio as gr
import numpy as np
import torch
from fastapi import FastAPI
from flask import Flask, Response, request, current_app, render_template, stream_with_context
from flask_cors import CORS
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import PolynomialFeatures
from tiktoken import get_encoding
from transformers import AutoModelForCausalLM, AutoTokenizer

from utils import *


# STEP1.åŠ è½½æ¨¡å‹
# åŒ…æ‹¬å¯¹è¯æ¨¡å‹å’ŒåµŒå…¥æ¨¡å‹ï¼Œå…¶ä¸­å¯¹è¯æ¨¡å‹ä»modelè·å–ï¼ŒåµŒå…¥æ¨¡å‹ä»datasetè·å–
# å¯¹è¯æ¨¡å‹å¿…é¡»ï¼ŒåµŒå…¥æ¨¡å‹å¯é€‰
# modelæ¨¡å‹ç±»åˆ«ä»model_type.txtæ–‡ä»¶ä¸­è·å–ï¼Œdatasetæ¨¡å‹ç±»åˆ«ä»å‹ç¼©æ–‡ä»¶åè·å–


def init_model_and_tokenizer() -> Tuple[AutoModelForCausalLM, AutoTokenizer]:
    """åˆå§‹åŒ–æ¨¡å‹å’Œè¯è¡¨"""
    my_model = AutoModelForCausalLM.from_pretrained(
        pretrained_model_name_or_path=path_eval_finetune,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    ).eval()
    my_tokenizer = AutoTokenizer.from_pretrained(
        pretrained_model_name_or_path=path_eval_finetune,
        use_fast=False,
        trust_remote_code=True
    )
    return my_model, my_tokenizer


def init_embeddings_model() -> Optional[SentenceTransformer]:
    """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
    for filename in listdir(path_eval_pretrain):
        modelname = match(pattern="(.*)\.zip", string=filename)  # noqa
        if modelname is not None:
            my_model_name = modelname.groups()[0]
            break
    else:
        return None
    system("unzip {}/{}.zip -d {}".format(path_eval_pretrain, my_model_name, path_eval_pretrain))
    my_model = SentenceTransformer(
        model_name_or_path="/dataset/m3e-large",
        device="cuda"  # noqa
    )
    return my_model


model, tokenizer = init_model_and_tokenizer()
embeddings_model = init_embeddings_model()


# STEP2.å¯åŠ¨æ¥å£æœåŠ¡
# æ¥å£æœåŠ¡å‡é‡‡ç”¨çº¿ç¨‹å¯åŠ¨ï¼Œé¿å…é˜»å¡
# é‡‡ç”¨frpå·¥å…·åå‘ä»£ç†


def init_api() -> Flask:
    """åˆ›å»ºæ¥å£æœåŠ¡"""
    my_api = Flask(import_name=__name__)  # å£°æ˜ä¸»æœåŠ¡
    CORS(app=my_api)  # å…è®¸è·¨åŸŸ
    return my_api


def init_frp() -> None:
    """åˆå§‹åŒ–frpå®¢æˆ·ç«¯"""
    system("chmod +x frpc/frpc")  # noqa
    system("nohup ./frpc/frpc -c frpc/frpc.ini &")  # noqa
    return


api = init_api()
Thread(target=api.run, kwargs={"host": appHost, "port": appPort, "debug": False}).start()
init_frp()


@api.route(rule="/", methods=["GET"])
def homepage():
    """æ¥å£æœåŠ¡é¦–é¡µ"""
    return render_template(template_name_or_list="Infinity.html")  # noqa


@api.route(rule="/v1/chat/completions", methods=["POST"])
def chat_completions() -> Response:
    """Chatæ¥å£"""
    req = ChatRequestSchema().load(request.json)
    return current_app.response_class(response=chat_stream(req=req), mimetype="text/event-stream")


@api.route(rule="/v1/embeddings", methods=["POST"])
def embeddings() -> str:
    """Embeddingsæ¥å£"""
    if embeddings_model is None:
        return ""
    req = EmbeddingsRequestSchema().load(request.json)
    result = [embeddings_model.encode(sentences=sentence) for sentence in req["input"]]
    # OpenAI API åµŒå…¥ç»´åº¦æ ‡å‡†1536
    result = [embeddings_pad(embedding=embedding, target_length=embeddingLength)
              if len(embedding) < embeddingLength else embedding for embedding in result]
    result = [embedding / np.linalg.norm(x=embedding) for embedding in result]
    result = [embedding.tolist() for embedding in result]
    prompt_tokens = sum(len(text.split()) for text in req["input"])
    total_tokens = sum(embeddings_token_num(text=text) for text in req["input"])
    data = [{"index": index, "embedding": embedding} for index, embedding in enumerate(result)]
    usage = {"prompt_tokens": prompt_tokens, "total_tokens": total_tokens}
    return EmbeddingsResponseSchema().dump({"model": req["model"], "data": data, "usage": usage})


@stream_with_context
def chat_stream(req: Dict):
    """æµå¼è¾“å‡ºæ¨¡å‹å›ç­”"""
    index = 0
    position = 0
    delta = ChatDeltaSchema().dump({"role": "assistant"})
    choice = ChatChoiceSchema().dump({"index": 0, "delta": delta, "finish_reason": None})
    yield chat_sse(line=ChatResponseSchema().dump({"model": req["model"], "choices": [choice]}))  # noqa
    # å¤šè½®å¯¹è¯ï¼Œæµå¼è¾“å‡º
    # æ¥å£ä½¿ç”¨å­—ç¬¦å¼
    for answer in model.chat(tokenizer, req["messages"], stream=True):  # noqa
        if torch.backends.mps.is_available():  # noqa
            torch.mps.empty_cache()  # noqa
        content = answer[position:]
        if not content:
            continue
        delta = ChatDeltaSchema().dump({"content": content})
        choice = ChatChoiceSchema().dump({"index": index, "delta": delta, "finish_reason": None})
        yield chat_sse(line=ChatResponseSchema().dump({"model": req["model"], "choices": [choice]}))  # noqa
        index += 1
        position = len(answer)
        if position > contentLength:
            break
    choice = ChatChoiceSchema().dump({"index": 0, "delta": {}, "finish_reason": "stop"})
    yield chat_sse(line=ChatResponseSchema().dump({"model": req["model"], "choices": [choice]}))  # noqa
    yield chat_sse(line="[DONE]")


def chat_sse(line: Union[str, Dict]) -> str:
    """Server Sent Events for stream"""
    return "data: {}\n\n".format(dumps(obj=line, ensure_ascii=False) if isinstance(line, dict) else line)


def embeddings_pad(embedding: np.ndarray, target_length: int) -> np.ndarray:
    """æŒ‰ç…§æŒ‡å®šç»´åº¦å¯¹åµŒå…¥å‘é‡è¿›è¡Œæ‰©ç¼©"""
    embedding = PolynomialFeatures(degree=2).fit_transform(X=embedding.reshape(1, -1))
    embedding = embedding.flatten()
    # ç»´åº¦å°å¡«å……ï¼Œç»´åº¦å¤§æˆªæ–­
    if len(embedding) < target_length:
        return np.pad(array=embedding, pad_width=(0, target_length - len(embedding)))
    return embedding[:target_length]


def embeddings_token_num(text: str) -> int:
    """è®¡ç®—åµŒå…¥æ¶ˆè€—"""
    return len(get_encoding(encoding_name="cl100k_base").encode(text=text))


# STEP3.å¯åŠ¨é¡µé¢æœåŠ¡
# AIåä½œå¹³å°ä¸é€‚ç”¨mainç©ºé—´æ‰§è¡Œï¼Œä¸”éœ€è¦ç”¨FastAPIæŒ‚è½½


def get_answer(chatbot: List[List[str]], textbox: str, history: List[Dict[str, str]]):  # noqa
    """æ¨¡å‹å›ç­”å¹¶æ›´æ–°èŠå¤©çª—å£"""
    chatbot.append([textbox, ""])
    history.append({"role": "user", "content": textbox})
    # å¤šè½®å¯¹è¯ï¼Œæµå¼è¾“å‡º
    # é¡µé¢ä½¿ç”¨æ®µè½å¼
    for answer in model.chat(tokenizer, history, stream=True):  # noqa
        if torch.backends.mps.is_available():  # noqa
            torch.mps.empty_cache()  # noqa
        chatbot[-1][1] = answer
        yield chatbot
        if len(answer) > contentLength:
            break
    history.append({"role": "assistant", "content": chatbot[-1][1]})


def clear_textbox() -> Dict:
    """æ¸…ç†ç”¨æˆ·è¾“å…¥ç©ºé—´"""
    return gr.update(value="")


def clear_chatbot_and_history(chatbot: List[List[str]], history: List[Dict[str, str]]) -> List:  # noqa
    """æ¸…ç†äººæœºå¯¹è¯å†å²è®°å½•"""
    chatbot.clear()
    history.clear()
    return chatbot


def init_demo() -> gr.Blocks:
    """åˆ›å»ºé¡µé¢æœåŠ¡"""
    with gr.Blocks(title="Infinity Model") as my_demo:
        # å¸ƒå±€åŒº
        gr.Markdown(value="<p align='center'>"
                          "<img src='https://openi.pcl.ac.cn/rhys2985/Infinity/raw/branch/master/templates/Infinity.png' "
                          "style='height: 100px'>"
                          "</p>")
        gr.Markdown(value="<center><font size=8>Infinity Chat Bot</center>")
        gr.Markdown(value="<center><font size=4>ğŸ˜¸ This Web UI is based on Infinity Model, developed by Rhys. ğŸ˜¸</center>")
        gr.Markdown(value="<center><font size=4>ğŸ”¥ <a href='https://openi.pcl.ac.cn/rhys2985/Infinity'>é¡¹ç›®åœ°å€</a> ğŸ”¥</center>")
        chatbot = gr.Chatbot(label="Infinity Model")  # noqa
        textbox = gr.Textbox(label="Input", lines=2)
        history = gr.State(value=[])
        with gr.Row():
            btnSubmit = gr.Button("Submit ğŸš€")
            btnClear = gr.Button("Clear ğŸ§¹")
        gr.Markdown(value="<center><font size=4>âš  I strongly advise you not to knowingly generate or spread harmful content, "
                          "including rumor, hatred, violence, reactionary, pornography, deception, etc. âš </center>")
        # åŠŸèƒ½åŒº
        btnSubmit.click(fn=get_answer, inputs=[chatbot, textbox, history], outputs=[chatbot])
        btnSubmit.click(fn=clear_textbox, inputs=[], outputs=[textbox])
        btnClear.click(fn=clear_chatbot_and_history, inputs=[chatbot, history], outputs=[chatbot])
    my_demo.queue()
    return my_demo


demo = init_demo()
# æ­£å¼ç¯å¢ƒå¯åŠ¨æ–¹æ³•
if __name__ == "__main__":
    demo.launch()
# AIåä½œå¹³å°å¯åŠ¨æ–¹æ³•
else:
    app = gr.mount_gradio_app(app=FastAPI(), blocks=demo, path=getenv("OPENI_GRADIO_URL"))  # noqa
